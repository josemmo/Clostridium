# -*- coding: utf-8 -*-
"""
Created on Thu Mar  9 17:06:31 2023

@author: Albert
"""

import numpy as np
from scipy import linalg
from sklearn.metrics import r2_score, auc, roc_curve
import multiprocessing as mp
import math
import scipy as sc

class LR_ARD(object):
    def __init__(self):
        pass

    def fit(self, z, y, z_tst = None, y_tst = None,  hyper = None, prune = 0, maxit = 15, 
            pruning_crit = 1e-6, tol = 1e-6):
        self.z = z  #(NxK)
        self.z_tst = z_tst  #(NxK_tst)
        self.y_tst = y_tst  #(NxD_tst)
        self.t_tst = y_tst
        self.y = y  #(NxD)
        self.t = y
        self.fact_sel = np.arange(self.z.shape[1])
        #self.K_tr = self.center_K(self.z @ self.z.T)
        self.K_tr = self.z @ self.z.T
        
        self.K = self.z.shape[1] #num dimensiones input
        self.D = self.t.shape[1] #num dimensiones output
        self.N = self.z.shape[0] # num datos
        self.N_tst = self.z_tst.shape[0]
        self.index = np.arange(self.K)
        
        # Some precomputed matrices
        #self.ZTZ = self.z.T @ self.z  #(KxK) es enorme, habrÃ­a que ver si se puede evitar este calculo
        #self.YTZ = self.y.T @ self.z  #(DxK) 
        self.KTK = self.K_tr.T @ self.K_tr
        self.KTY = self.K_tr.T @ self.y
        self.YTK = self.t.T @ self.K_tr
        self.L = []
        self.mse = []
        self.mse_tst = []        
        self.R2 = []
        self.R2_tst = []
        #self.AUC = []
        #self.AUC_tst = []
        self.K_vec = []
        self.labels_pred = []
        self.input_idx = np.ones(self.K, bool)
        if hyper == None:
            self.hyper = HyperParameters(self.K, self.N)
        else:
            self.hyper = hyper
        self.q_dist = Qdistribution(self.N, self.D, self.K, self.hyper)

        self.fit_vb(prune, maxit, pruning_crit, tol)
        
        
    def center_K(self, K):
        """Center a kernel matrix K, i.e., removes the data mean in the feature space
        Args:
            K: kernel matrix
        """
           
        size_1,size_2 = K.shape;
        D1 = K.sum(axis=0)/size_1
        D2 = K.sum(axis=1)/size_2
        E = D2.sum(axis=0)/size_1
        return K + np.tile(E,[size_1,size_2]) - np.tile(D1,[size_1,1]) - np.tile(D2,[size_2,1]).T


    def pruning(self, pruning_crit):
        q = self.q_dist
        
        maximo = np.max(abs(self.z.T @ q.A['mean']))
        
        self.fact_sel = np.arange(self.z.shape[1])[(abs(self.z.T @ q.A['mean']) > maximo*pruning_crit).flatten()].astype(int)
        
        aux = self.input_idx[self.input_idx]
        aux[self.fact_sel] = False
        self.input_idx[self.input_idx] = ~aux
        
        
        self.z = self.z[:,self.fact_sel]
        self.z_tst = self.z_tst[:,self.fact_sel]
        self.K_tr = self.z @ self.z.T
        q.alpha['a'] = q.alpha['a'][self.fact_sel]
        q.alpha['b'] = q.alpha['b'][self.fact_sel]
        self.hyper.alpha_a = self.hyper.alpha_a[self.fact_sel]
        self.hyper.alpha_b = self.hyper.alpha_b[self.fact_sel]
        self.index = self.index[self.fact_sel]
        

#    def pruning(self, pruning_crit):       
#        q = self.q_dist
#        
#        fact_sel = np.arange(self.z.shape[1])[(abs(np.diagflat(q.V['mean']) @ self.z.T @ q.A['mean'].T) > pruning_crit).flatten()].astype(int)
#        
#        aux = self.input_idx[self.input_idx]
#        aux[fact_sel] = False
#        self.input_idx[self.input_idx] = ~aux
#        
#        # Pruning alpha
#        self.z = self.z[:,fact_sel]
#        self.z_tst = self.z_tst[:,fact_sel]
#        q.V['mean'] = q.V['mean'][fact_sel]
#        q.V['cov'] = q.V['cov'][fact_sel, fact_sel]
#        q.alpha['a'] = q.alpha['a'][fact_sel]
#        q.alpha['b'] = q.alpha['b'][fact_sel]
#        self.hyper.alpha_a = self.hyper.alpha_a[fact_sel]
#        self.hyper.alpha_b = self.hyper.alpha_b[fact_sel]
#        self.index = self.index[fact_sel]
##        q.alpha['a'] = q.alpha['a']
##        q.alpha['b'] = q.alpha['b']
##        self.hyper.alpha_a = self.hyper.alpha_a
##        self.hyper.alpha_b = self.hyper.alpha_b
#        q.K = len(fact_sel)
    
    
    def compute_mse(self, z = None, y = None):
        q = self.q_dist
        if z is None:
            z = self.z_tst
        if y is None:
            y = self.y_tst
        diff = (y - z @ self.z.T @ q.A['mean']).ravel()
        return  diff@diff/self.N
    
    def compute_R2(self, z = None, y = None):
        q = self.q_dist
        if z is None:
            z = self.z_tst
        if y is None:
            y = self.y_tst
        return  r2_score(y.ravel(), (z @ self.z.T @ q.A['mean']).ravel())
        
    def predict(self, Z_test):
        q = self.q_dist
        if Z_test == None:
            Z_test = self.z_tst
        return Z_test @ self.z.T @ q.A['mean']
    
    def predict_proba(self, Z_test):
        q = self.q_dist
        probs = np.zeros((np.shape(Z_test)[0],1))
        for i in range(np.shape(Z_test)[0]):
            mean = Z_test[np.newaxis,i,:] @ self.z.T @ q.A['mean']
            sig = q.tau_mean() + Z_test[np.newaxis,i,:] @ self.z.T @ q.A['cov'] @ self.z @ Z_test[np.newaxis,i,:].T
            prob = self.sigmoid(mean/(np.sqrt(1+(np.pi/8)*sig)))
            probs[i,0] = prob
        return probs
    
    def predict_proba_th(self, Z_test, pruning_crit):
        q = self.q_dist
        maximo = np.max(abs(self.z.T @ q.A['mean']))
        fact = np.arange(self.z.shape[1])[(abs(self.z.T @ q.A['mean']) > maximo*pruning_crit).flatten()].astype(int)
        z = self.z.copy()
        z = z[:,fact]
        Z_test = Z_test[:,fact]

        probs = np.zeros((np.shape(Z_test)[0],1))
        for i in range(np.shape(Z_test)[0]):
            mean = Z_test[np.newaxis,i,:] @ z.T @ q.A['mean']
            sig = q.tau_mean() + Z_test[np.newaxis,i,:] @ z.T @ q.A['cov'] @ z @ Z_test[np.newaxis,i,:].T
            prob = self.sigmoid(mean/(np.sqrt(1+(np.pi/8)*sig)))
            probs[i,0] = prob
        return probs

    
    def return_a(self):
        q = self.q_dist
        return q.A['mean']
    
    def return_alpha(self):
        q = self.q_dist
        return q.alpha_mean()
    
    def return_w(self):
        q = self.q_dist
        return self.z.T @ q.A['mean']
       
    def return_pred(self):
        q = self.q_dist
        
        probs = np.zeros((np.shape(self.z_tst)[0],1))
        sigs = np.zeros((np.shape(self.z_tst)[0],1))
        for i in range(np.shape(self.z_tst)[0]):
            mean = self.z_tst[np.newaxis,i,:] @ self.z.T @ q.A['mean']
            sig = q.tau_mean() + self.z_tst[np.newaxis,i,:] @ self.z.T @ q.A['cov'] @ self.z @ self.z_tst[np.newaxis,i,:].T
            prob = self.sigmoid(mean/(np.sqrt(1+(np.pi/8)*sig)))
            probs[i,0] = prob
            sigs[i,0] = sig
        pred = np.where(probs.ravel()>0.5,1,0)
        return pred, sigs
    
    
    def return_fact_sel(self):
        q = self.q_dist
        return self.fact_sel
    
    def return_index(self):
        q = self.q_dist
        return self.index
    
    def return_proba(self):
        q = self.q_dist
        
        probs = np.zeros((np.shape(self.z_tst)[0],1))
        # inv = self.myInverse(q.A['cov']) 
        for i in range(np.shape(self.z_tst)[0]):
            mean = self.z_tst[np.newaxis,i,:] @ self.z.T @ q.A['mean']
            sig = q.tau_mean() + self.z_tst[np.newaxis,i,:] @ self.z.T @ q.A['cov'] @ self.z @ self.z_tst[np.newaxis,i,:].T
            # sig = 1/q.tau_mean() + self.z_tst[np.newaxis,i,:] @ self.z.T @ q.A['cov'] @ self.z @ self.z_tst[np.newaxis,i,:].T
            prob = self.sigmoid(mean/(np.sqrt(1+(np.pi/8)*sig)))
            probs[i,0] = prob
        return probs
    
    def return_proba_th(self, pruning_crit):
        q = self.q_dist
        maximo = np.max(abs(self.z.T @ q.A['mean']))
        fact = np.arange(self.z.shape[1])[(abs(self.z.T @ q.A['mean']) > maximo*pruning_crit).flatten()].astype(int)
        self.z = self.z[:,fact]
        self.z_tst = self.z_tst[:,fact]

        probs = np.zeros((np.shape(self.z_tst)[0],1))
        for i in range(np.shape(self.z_tst)[0]):
            mean = self.z_tst[np.newaxis,i,:] @ self.z.T @ q.A['mean']
            sig = q.tau_mean() + self.z_tst[np.newaxis,i,:] @ self.z.T @ q.A['cov'] @ self.z @ self.z_tst[np.newaxis,i,:].T
            # sig = 1/q.tau_mean() + self.z_tst[np.newaxis,i,:] @ self.z.T @ q.A['cov'] @ self.z @ self.z_tst[np.newaxis,i,:].T
            prob = self.sigmoid(mean/(np.sqrt(1+(np.pi/8)*sig)))
            probs[i,0] = prob
        return probs


    
    def return_proba_train(self):
        q = self.q_dist
        
        probs = np.zeros((np.shape(self.z)[0],1))
        for i in range(np.shape(self.z)[0]):
            mean = self.z[np.newaxis,i,:] @ self.z.T @ q.A['mean']
            sig = q.tau_mean() + self.z[np.newaxis,i,:] @ self.z.T @ q.A['cov'] @ self.z @ self.z[np.newaxis,i,:].T
            prob = self.sigmoid(mean/(np.sqrt(1+(np.pi/8)*sig)))
            probs[i,0] = prob
        return probs
        
    def sigmoid(self,x):
        if any(x < 0):
            return np.exp(x)/(1 + np.exp(x))
        else:
            return 1/(1 + np.exp(-x))
    
    def fit_vb(self, prune, maxit=30, pruning_crit = 1e-1, tol = 1e-6):
        q = self.q_dist
        for i in range(maxit):
            self.update()

            ##############
            #self.labels_pred.append(self.predict(self.z_tst))
            
            #print('MSE: ',self.compute_mse(self.z_tst,self.y_tst))
            ##############
            print(i)
            self.L.append(self.update_bound())
            print('Features: ',np.shape(self.z)[1])
            if prune == 1 and i>3:
                self.pruning(pruning_crit)
            ##################
            ##################
            #print('\rIteration %d Lower Bound %.1f K %4d' %(i+1, self.L[-1]), end='\r', flush=True)
            if (len(self.L) > 100) and (abs(1 - np.mean(self.L[-101:-1])/self.L[-1]) < tol):
               print('\nModel correctly trained. Convergence achieved')
               return 
            print('LB: ',self.L[-1])               
        print('')

    def update(self):

        self.update_a()
        self.update_alpha()
        self.update_y()
        self.update_xi()
        self.update_tau()

    def myInverse(self,X):
        """Computation of the inverse of a matrix.
        
        This function calculates the inverse of a matrix in an efficient way 
        using the Cholesky decomposition.
        
        Parameters
        ----------
        __A: bool, (default 0). 
            Whether or not to print all the lower bound updates.
            
        """
        
        # try:
        #     L = linalg.pinv(np.linalg.cholesky(X), rcond=1e-10) #np.linalg.cholesky(A)
        #     return np.dot(L.T,L) #linalg.pinv(L)*linalg.pinv(L.T)
        # except:
        #     return np.nan
        try:
            return linalg.pinv(X)
        except:
            return np.nan
        
    
    def update_a(self):
        q = self.q_dist
        
        a_cov = self.z @ np.diagflat(q.alpha_mean()) @ self.z.T + q.tau_mean() * (self.K_tr.T @ self.K_tr)
        a_cov_inv = self.myInverse(a_cov)
        
        if not np.any(np.isnan(a_cov_inv)):
            q.A['cov'] = a_cov_inv
            ##############
            q.A['mean'] = q.tau_mean() * q.A['cov'] @ self.K_tr.T @ q.Y['mean']
            #############
            q.A['prodT'] = q.A['mean'] @ q.A['mean'].T + q.A['cov']
        else:
            print('Covariance of A not invertible, not Updated')
    
    def update_alpha(self):
        q = self.q_dist

        q.alpha['a'] = (self.hyper.alpha_a + 0.5)
        q.alpha['b'] = (self.hyper.alpha_b + 0.5 * np.diag(self.z.T @ q.A['prodT'] @ self.z))
    
    def update_tau(self):
        q = self.q_dist
        
        q.tau['a'] = self.N*0.5 + self.hyper.tau_a
        
        q.tau['b'] = 0.5*(np.trace(q.Y['prodT']) - 2*np.trace(q.Y['mean'].T @ self.K_tr @ q.A['mean']) + np.trace(self.K_tr.T @ self.K_tr @ q.A['prodT']))+self.hyper.tau_b
    
    def update_y(self):
        q = self.q_dist
        y_cov = q.tau_mean()*np.eye(self.N) + 2*np.diagflat(q.xi['mean'])
        y_cov_inv = self.myInverse(y_cov)
        
        if not np.any(np.isnan(y_cov_inv)):
            q.Y['cov'] = y_cov_inv
            
            
            #q.Y['mean'] = q.Y['cov'] @ (self.t - 0.5*np.ones((self.N,1)) + q.tau_mean() * self.z @ self.z.T @ q.A['mean'])

            q.Y['mean'] = q.Y['cov'] @ (self.t - 0.5*np.ones((self.N,1)) + q.tau_mean() * self.K_tr @ q.A['mean'])
            
            q.Y['prodT'] = q.Y['mean'] @ q.Y['mean'].T + q.Y['cov']
        else:
            print('Covariance of Y not invertible, not Updated')
    
    def update_xi(self):
        q = self.q_dist
        q.xi['mean'] = (q.Y['mean'].reshape((self.N,)))**2 + np.diag(q.Y['cov'])
            
    def HGamma(self, a, b):
        """Compute the entropy of a Gamma distribution.

        Parameters
        ----------
        __a: float. 
            The parameter a of a Gamma distribution.
        __b: float. 
            The parameter b of a Gamma distribution.

        """
        
        return -np.log(b)
    
    def HGauss(self, mn, cov, entr):
        """Compute the entropy of a Gamma distribution.
        
        Uses slogdet function to avoid numeric problems. If there is any 
        infinity, doesn't update the entropy.
        
        Parameters
        ----------
        __mean: float. 
            The parameter mean of a Gamma distribution.
        __covariance: float. 
            The parameter covariance of a Gamma distribution.
        __entropy: float.
            The entropy of the previous update. 

        """
        
        H = 0.5*mn.shape[0]*np.linalg.slogdet(cov)[1]
        return self.checkInfinity(H, entr)
        
    def checkInfinity(self, H, entr):
        """Checks if three is any infinity in th entropy.
        
        Goes through the input matrix H and checks if there is any infinity.
        If there is it is not updated, if there isn't it is.
        
        Parameters
        ----------
        __entropy: float.
            The entropy of the previous update. 

        """
        
        if abs(H) == np.inf:
            return entr
        else:
            return H
        
    def update_bound(self):
        """Update the Lower Bound.
        
        Uses the learnt variables of the model to update the lower bound.
        
        """
        
        q = self.q_dist
        q.A['LH'] = self.HGauss(q.A['mean'], q.A['cov'], q.A['LH'])
#        q.V['LH'] = self.HGauss(q.V['mean'], np.diagflat(q.V['cov']), q.V['LH'])
        #lel = self.HGauss(q.V['mean'], np.diagflat(q.V['cov']), q.V['LH'])
        #q.b['LH'] = self.HGauss(q.b['mean'], q.b['cov'], q.b['LH'])
        #q.W['LH'] = self.HGauss(q.W['mean'], q.W['cov'], q.W['LH'])
        #self.W['LH'] = self.z.T @ q.A['LH']
        # Entropy of alpha and tau
        # q.alpha['LH'] = np.sum(self.HGamma(q.alpha['a'], q.alpha['b']))
        # q.tau['LH'] = np.sum(self.HGamma(q.tau['a'], q.tau['b']))
            
        # Total entropy
        # EntropyQ = q.W['LH'] + q.alpha['LH']  + q.tau['LH']
           
        # Calculation of the E[log(p(Theta))]
        
        q.tau['ElogpWalp'] = -(0.5 *  self.N + self.hyper.tau_a - 2)* np.log(q.tau['b'])
        q.alpha['Elogp'] = -(0.5 + np.mean(self.hyper.alpha_a) - 2)* np.sum(np.log(q.alpha['b']))
        
        # Total E[log(p(Theta))]
        ElogP = q.tau['ElogpWalp'] + q.alpha['Elogp'] 
        return ElogP - q.A['LH']
    
#    def update_truncated(self, mu, sig):
#        alfa_fi = (1/(np.sqrt(2*np.pi)))*np.exp((mu**2)/(2*(sig)))
#        alfa_FI = (1/2)*(1 + math.erf(-((mu)/np.sqrt(sig*2))))
#        
#        sigma = sig*(1 + (-((mu/np.sqrt(sig))*alfa_fi)/(1-alfa_FI)) - (alfa_fi/(1-alfa_FI))**2)
#        mean = mu + (alfa_fi/(1-alfa_FI))*sig
#        
#        return mean, sigma
               
    def update_abs(self, mea, sig):
        mean = np.sqrt((sig*2)/(np.pi))*np.exp(-(mea**2)/(2*(sig))) + mea*(1 - 2*sc.stats.norm.cdf(-(mea/np.sqrt(sig))))
        sigma = sig + mea**2 - mean**2
        return mean, sigma
        

class HyperParameters(object):
    def __init__(self, K, N):
        #self.alpha_a = 2 * np.ones((K,))
        #self.alpha_b = 1 * np.ones((K,))
        self.alpha_a = 1000 * np.ones((K,))
        #self.alpha_a = 100000 * np.ones((K,))
        self.alpha_b = 1 * np.ones((K,))

        #self.alpha_a = 0.1 * np.ones((K,))
        #self.alpha_b = 0.01 * np.ones((K,))

        #self.alpha_a = 1e-12 * np.ones((K,))
        #self.alpha_b = 1e-14 * np.ones((K,))

        #self.tau_a = 1e-12
        #self.tau_b = 1e-14
        self.tau_a = 1e-14
        self.tau_b = 1e-14


class Qdistribution(object):
    def __init__(self, n, D, K, hyper):
        self.n = n
        self.D = D
        self.K = K
        
        # Initialize gamma disributions
        alpha = self.qGamma(hyper.alpha_a,hyper.alpha_b,self.K)
        self.alpha = alpha 
        tau = self.qGamma(hyper.tau_a,hyper.tau_b,1)
        self.tau = tau 

        # The remaning parameters at random
        self.init_rnd()

    def init_rnd(self):
        self.A = {
                "mean":     None,
                "cov":      None,
                "prodT":    None,
                "LH":       0,
                "Elogp":    0,
            }
        
        self.Y = {
                "mean":     None,
                "cov":      None,
                "prodT":    None,
                "LH":       0,
                "Elogp":    0,
            }
        
        self.xi = {
                "mean":     None,
                "cov":      None,
                "prodT":    None,
                "LH":       0,
                "Elogp":    0,
            }
            
#        self.W["mean"] = np.random.normal(0.0, 1.0, self.D * self.K).reshape(self.D, self.K)
#        self.W["cov"] = np.eye(self.K)
#        self.W["prodT"] = np.dot(self.W["mean"].T, self.W["mean"])+self.K*self.W["cov"]
        
        
        
        #self.A["mean"] = np.random.normal(0.0, 1.0, self.D * self.K).reshape(self.D, self.K)
        #np.random.seed(41)
        self.A["mean"] = np.random.normal(0.0, 1.0, self.n).reshape(self.n,1)
        #self.A["cov"] = np.eye(self.K)
        self.A["cov"] = np.eye(self.n)
        #self.A["prodT"] = np.dot(self.A["mean"].T, self.A["mean"])+self.K*self.A["cov"]
        self.A["prodT"] = self.A["mean"].T @ self.A["mean"]+self.n*self.A["cov"]
        
        #Inicializamos las Y
        self.Y["mean"] = (1/2)*np.ones((self.n,1))
        self.Y["cov"] = np.eye(self.n)
        self.Y["prodT"] = self.Y["mean"].T @ self.Y["mean"]+self.n*self.Y["cov"]
        
        #Inicializamos Xi
        self.xi["mean"] = np.ones((self.n,1))
        

    def qGamma(self,a,b,K):
        """ Initialisation of variables with Gamma distribution..
    
        Parameters
        ----------
        __a : array (shape = [1, 1]).
            Initialistaion of the parameter a.        
        __b : array (shape = [K, 1]).
            Initialistaion of the parameter b.
        __m_i: int.
            Number of views. 
        __K: array (shape = [K, 1]).
            dimension of the parameter b for each view.
            
        """
        
        param = {                
                "a":         a,
                "b":         b,
                "LH":         None,
                "ElogpWalp":  None,
            }
        return param
        
    
    def alpha_mean(self):
        return self.alpha['a'] / self.alpha['b'] 
    def tau_mean(self):
        return self.tau['a'] / self.tau['b']

    
